{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aaf3791",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Korpusverarbeitung ‚Äì Annotation mit spaCy\n",
    "\n",
    "## √úbersicht\n",
    "Im Folgenden wird exemplarisch ein Text (txt-Datei) mit der Bibliothek [spaCy](https://spacy.io) annotiert. Daf√ºr werden folgendene Schritte durchgef√ºhrt:\n",
    "1. Einlesen des Texts\n",
    "2. Worth√§ufigkeiten ohne echte Tokenisierung\n",
    "   * Aufteilen des Texts in W√∂rter auf Grundlage von Leerzeichen\n",
    "   * Abfrage von H√§ufigkeiten\n",
    "4. Annotation mit spaCy\n",
    "   * Laden des Sprachmodells\n",
    "   * Analysekomponenten ausw√§hlen\n",
    "   * Text annotieren\n",
    "   * Worth√§ufigkeiten anzeigen\n",
    "5. Annotation speichern\n",
    "6. Prozess f√ºr das gesamte Korpus ausf√ºhren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da21b95-0aa9-43e0-8799-0adc631ebf9c",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary><b>Informationen zum Ausf√ºhren des Notebooks ‚Äì Zum Ausklappen klicken ‚¨áÔ∏è</b></summary>\n",
    "  \n",
    "<b>Voraussetzungen zur Ausf√ºhrung des Jupyter Notebooks</b>\n",
    "<ol>\n",
    "<li> Installieren der Bibliotheken </li>\n",
    "<li>2. Laden der Daten (z.B. √ºber den Command `wget` (s.u.))</li>\n",
    "<li>3. Pfad zu den Daten setzen</li>\n",
    "</ol>\n",
    "Zum Testen: Ausf√ºhren der Zelle \"load libraries\" und der Sektion \"Einlesen des Texts\". </br>\n",
    "Alle Zellen, die mit üöÄ gekennzeichnet sind, werden nur bei der Ausf√ºhrung des Noteboos in Colab / JupyterHub bzw. lokal ausgef√ºhrt. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c15d4a-f4ed-48e8-b4db-fdf8f60d2507",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "#  üöÄ Install libraries \n",
    "! pip install tqdm pandas numpy spacy bokeh \n",
    "\n",
    "#  üöÄ Load german language model for annotation\n",
    "! python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d33ac2-97dd-45fd-99b5-dac91fdde243",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# load libraries \n",
    "import json\n",
    "import typing\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import CustomJS, TextInput, Div"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81998d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Einlesen des Texts\n",
    "Um eine Datei mit Python bearbeiten zu k√∂nnen, muss die Datei zuerst ausgew√§hlt werden, d.h der [Pfad](https://en.wikipedia.org/wiki/Path_(computing)) zur Datei wird gesetzt und dann eingelesen werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bed39-3c81-4caa-972f-cbc68471b6ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "<details>\n",
    "  <summary><b>Informationen zum Ausf√ºhren des Notebooks ‚Äì Zum Ausklappen klicken ‚¨áÔ∏è</b></summary>\n",
    "Zuerst wird der Ordner angelegt, in dem die Textdateien gespeichert werden. Der Einfachheit halber wird die gleich Datenablagestruktur wie in dem <a href=\"https://github.com/dh-network/quadriga/tree/main\">GitHub Repository</a>, in dem die Daten gespeichert sind, vorausgesetzt. </br>\n",
    "Der Text wird aus GitHub heruntergeladen und in dem Ordner <i>../data/txt/</i> abgespeichert. </br>\n",
    "Der Pfad kann in der Variable <i>text_path</i> angepasst werden. Die einzulesenden Daten m√ºssen die Endung `.txt` haben. </br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ccb971-f3b8-4b1a-851a-786fc86a9ace",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ Create data directory path\n",
    "corpus_dir = Path(\"../data/txt\")\n",
    "if not corpus_dir.exists():\n",
    "    corpus_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da277ff-8e06-46b7-8ae3-2af076036d37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ Load the txt file from GitHub \n",
    "! wget https://raw.githubusercontent.com/dh-network/quadriga/refs/heads/main/data/txt/SNP2719372X-19181015-0-0-0-0.txt\n",
    "\n",
    "# move the file to the data directory\n",
    "! mv SNP2719372X-19181015-0-0-0-0.txt ../data/txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5cbf8-a9dc-4788-bd0e-f95bdfa57e64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the path to file to be processed\n",
    "text_path = Path(\"../data/txt/SNP2719372X-19181015-0-0-0-0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa6bd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read text and print some parts of the text\n",
    "if text_path.is_file():\n",
    "    text = text_path.read_text()\n",
    "    print(f\"Textauszug:\\n {text[10280:10400]}\")\n",
    "else:\n",
    "    print(\"The file path does not exist. Set the variable text_path to an existing path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d22bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Worth√§ufigkeiten ohne echte Tokenisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d76a71-2035-4e50-ab55-7604e60deef7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.1 Text in W√∂rter aufteilen\n",
    "Der einfachste Weg einen Text automatisch in W√∂rter aufzuteilen, ist anzunehmen, das W√∂rter durch Leerzeichen getrennt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07258cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the text into words by space\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694298bd-8e8f-4570-b053-f3e7b42f77d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Pr√ºfen**: Wie sieht die Wortliste aus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37fa42-ae7a-4bca-8b01-36cd1444e31a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the 100th up the 120th words\n",
    "words[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d542d5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Wie viele W√∂rter gibt es insgesamt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee91df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the length of the word list\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4e5d1-4a04-44e9-9acb-d178549ef8a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Wie zu sehen ist, hat diese Art der \"falschen\" Tokenisierung den Nachteil, dass Satzzeichen nicht von W√∂rtern abgetrennt werden. \\\n",
    "Die Wortanzahl ist dementsprechend auch nicht akkurat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca89700-b85b-4666-8a2e-0e6fba45d257",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.2 Anzeigen von Worth√§ufigkeiten\n",
    "Auf Grundlage dieser Wortliste kann trotzdem schon eine erste basale H√§ufigkeitenabfrage erfolgen. Daf√ºr werden die W√∂rter zuerst gez√§hlt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbc10d-78ed-4cba-9927-c2c145db18bf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the words with Counter and save the result to a variable\n",
    "word_frequencies = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621342ad-0a89-4066-8b49-d8f89250ae7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "<details>\n",
    "  <summary><b>Informationen zum Ausf√ºhren des Notebooks ‚Äì Zum Ausklappen klicken ‚¨áÔ∏è</b></summary>\n",
    "Um die H√§ufigkeit nur mit Python abzufragen, kann folgende Zeile ausgef√ºhrt werden:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea0f435-d0c8-4409-9c55-6c29514c0b29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ get the number of the word \"Grippe\" in the word frequencies \n",
    "word_frequencies[\"Grippe\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3685447-ba0b-4c50-9490-99148bc743e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Dann kann die H√§ufigkeit abgefragt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d2cc1-15b8-457e-82d5-7b27e37e7a7b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Ensure Bokeh output is displayed in the notebook\n",
    "output_notebook()\n",
    "\n",
    "# Convert the dictionary to a JSON string to be passed to javascript\n",
    "word_freq_json = json.dumps(word_frequencies)\n",
    "\n",
    "# Create the text input widget\n",
    "text_input = TextInput(value='', title=\"Geben Sie ein Wort ein:\")\n",
    "\n",
    "# Create a Div to display the frequency\n",
    "frequency_display = Div(text=\"H√§ufigkeit: \")\n",
    "\n",
    "# JavaScript callback to update the frequency display\n",
    "# Only needed for graphical interface \n",
    "callback = CustomJS(args=dict(frequency_display=frequency_display, text_input=text_input), code=f\"\"\"\n",
    "    var word = text_input.value.trim();\n",
    "    console.log('Input value:', word);\n",
    "\n",
    "    // Parse the word frequency dictionary from Python\n",
    "    var word_freq = {word_freq_json};\n",
    "\n",
    "    var frequency = word in word_freq ? word_freq[word] : \"not found\";\n",
    "    frequency_display.text = \"H√§ufigkeit: \" + frequency;\n",
    "\"\"\")\n",
    "\n",
    "text_input.js_on_change('value', callback)\n",
    "\n",
    "# Layout and display\n",
    "layout = column(text_input, frequency_display)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b158ecb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. Annotation mit spaCy\n",
    "Um eine pr√§zisere Einteilung in W√∂rter zu erhalten (Tokenisierung) und um flektierte W√∂rter aufeinander abbildbar zu machen (Lemmatisierung), wird der Text im folgenden durch die Bibliothek [spaCy](https://spacy.io/) annotiert. Daf√ºr werden folgende Schritte ausgef√ºhrt:\n",
    "1. Das sprachspezifische Modell wird geladen. Wir arbeiten mit dem weniger akkuraten aber schnellsten spaCy Modell `de_core_news_sm`. \n",
    "2. F√ºr eine erh√∂hte Annotationsgeschwindigkeit werden nur bestimmte Analysekomponenten geladen. Dies ist vor allem f√ºr gr√∂√üere Textmengen sinnvoll.\n",
    "3. Der Text wird annotiert und die Token sowie die dazugeh√∂rigen Lemmata werden extrahiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69caa926-5e4f-4a7f-bc27-3b4967c4b3e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.1 Sprachmodell laden\n",
    "Das sprachspezifische Modell wird geladen. Es handelt sich dabei um das am wenigsten akkurate aber schnellste Modell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52bea2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a88c3-4beb-42ea-9f88-290a92ce7cc0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.2 Analysekomponenten ausw√§hlen\n",
    "Es werden einige Analysekomponent wie z. B. das Aufteilen des Texts in S√§tze (sentencizer) oder die [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) (ner) ausgeschlossen, da diese f√ºr die Tokenisierung und die Lemmatisierung nicht ben√∂tigt werden. Der Auschluss der Komponentnen erh√∂ht die Annotationsgeschwindikgeit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb49918",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "disable_components = ['ner', 'morphologizer', 'attribute_ruler', 'sentencizer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bec70c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.3 Annotieren der Texte: Token, Lemma\n",
    "Der ausgew√§hlte Text wird mit spaCy annotiert und die Token sowie die dazugeh√∂rigen Lemmata werden extrahiert und in einer Tabelle gespeichert. Das Tabellenformat wurde gew√§hlt, da sich darin gut relationale Daten speichern lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800a679-11c9-4aff-a509-ccc63d9fb6e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the current time to display how long the annotation took\n",
    "current = time()\n",
    "\n",
    "# annotate with spacy\n",
    "doc = nlp(text)\n",
    "\n",
    "# extract tokens and lemmata, save them to a dictionary\n",
    "text_annotated = {}\n",
    "text_annotated['Token'] = [tok.text for tok in doc]\n",
    "text_annotated['Lemma'] = [tok.lemma_ for tok in doc]\n",
    "\n",
    "# convert the dictionary to a dataframe \n",
    "text_annotated_df = pd.DataFrame(text_annotated)\n",
    "\n",
    "# calculate how long the annotation and extraction took and print result\n",
    "took = time() - current\n",
    "print(f\"Die Annotation hat {round(took, 2)} Sekunden gedauert.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccb8ff-e04c-4ad1-b00c-5ffd35b9f873",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Auszug aus der Tabelle, in der der annotierte Text gespeichert ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a351b8d-a2eb-4f6e-95c8-73b6ba0daed5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print first five lines of the annotation\n",
    "text_annotated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8160c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3.4 Worth√§ufigkeit mit echter Tokenization   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf9300-6a9b-41f7-8b4b-3fbfb1adea04",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Durch die Tokenisierung wurden z. B. Satzzeichen von W√∂rtern abgetrennt. An der Textl√§nge l√§sst sich dies schon erkennen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a224b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the lemmata \n",
    "text_tokenized = text_annotated_df.Lemma\n",
    "\n",
    "# print the length\n",
    "len(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640ab432-cfce-4599-ac77-35d72176cca4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Auf Grundlage des tokenisierten und lemmatisierten Texts, kann die H√§ufigkeitenabfrage erneut augef√ºhrt werden. Da durch die Lemmatisierung flektierte Wortformen auf die Grundformen zur√ºckgef√ºhrt wurden, erwarten wir, dass die H√§ufigkeit einer Wortgrundform im Gegensatz zur vorherigen Abfrage erh√∂ht ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d142db-b63f-41ba-8651-6ef8faeb9d6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the words with Counter and save the result to a variable\n",
    "token_frequencies = Counter(text_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e5550-a2bd-4553-a253-0826009abe53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "<details>\n",
    "  <summary><b>Informationen zum Ausf√ºhren des Notebooks ‚Äì Zum Ausklappen klicken ‚¨áÔ∏è</b></summary>\n",
    "Um die H√§ufigkeit nur mit Python abzufragen, kann folgende Zeile ausgef√ºhrt werden:\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baeaaf5-57a2-4b7e-ad03-2ee1b525016b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell",
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ get the number of the word \"Grippe\" in the word frequencies \n",
    "token_frequencies[\"Grippe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f710e18-9fbd-4e9e-8b70-09181b751a70",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Ensure Bokeh output is displayed in the notebook\n",
    "output_notebook()\n",
    "\n",
    "# Convert the dictionary to a JSON string\n",
    "tok_freq_json = json.dumps(token_frequencies)\n",
    "\n",
    "# Create the text input widget\n",
    "token_input = TextInput(value='', title=\"Geben Sie ein Wort ein:\")\n",
    "\n",
    "# Create a Div to display the frequency\n",
    "token_frequency_display = Div(text=\"H√§ufigkeit: \")\n",
    "\n",
    "# JavaScript callback to update the frequency display\n",
    "# Only needed for graphical interface \n",
    "tok_callback = CustomJS(args=dict(frequency_display=token_frequency_display, text_input=token_input), code=f\"\"\"\n",
    "    var tok = text_input.value.trim();\n",
    "    console.log('Input value:', tok);\n",
    "\n",
    "    // Parse the word frequency dictionary from Python\n",
    "    var word_freq = {tok_freq_json};\n",
    "\n",
    "    var frequency = tok in word_freq ? word_freq[tok] : \"0\";\n",
    "    frequency_display.text = \"H√§ufigkeit: \" + frequency;\n",
    "\"\"\")\n",
    "\n",
    "token_input.js_on_change('value', tok_callback)\n",
    "\n",
    "# Layout and display\n",
    "layout = column(token_input, token_frequency_display)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402923a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. Annotation speichern\n",
    "Um den annotierten Text zu speichern, wird zuerst der Dateiname festgelegt. Daf√ºr wird die Dateiendung ersetzt von `.txt` zu `.csv`.\n",
    "\n",
    "[CSV](https://en.wikipedia.org/wiki/Comma-separated_values) (comma-separated value) ist das Standardformat um tabellarische Daten im Klartext zu speichern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb07287-60c4-4b33-8e10-4af7ca4ecbc9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "<details>\n",
    "  <summary><b>Informationen zum Ausf√ºhren des Notebooks</b></summary>\n",
    "Der Pfad zum Schreiben der Ergebnisse wird hier auf den selben Ordner gesetzt, in dem das Notebook liegt. So wird nicht von einer bestimmten Ordner-Struktur ausgegangen, wie in der Code-Zeile danach. Dort wird davon ausgeganen, dass auf der selben H√∂he des Ordners, in dem das Notebook liegt, ein Ordner `data` existiert, in dem ein Ordner `csv` vorhanden ist. In dem Ordner `csv` wird die Annotation gespeichert. </br></br>\n",
    "‚ö†Ô∏è Die n√§chste Zeile, in der der Pfad noch einmal gesetzt wird, muss √ºbersprungen werden.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90954af4-2137-4715-b967-9c4c0706ae6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set output path to current directory\n",
    "output_path = Path.cwd() / text_path.with_suffix(\".csv\").name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e926f2d-d06b-4aa7-b6f4-cd7d64b25520",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set output path, change file extension\n",
    "output_path = Path(r\"../data/csv\") / text_path.with_suffix(\".csv\").name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddac84a-5fbe-4304-9023-b4c0e18ab76e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Der Text wird dann unter dem festgelegten Dateinamen gespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc1fab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the annotation as csv\n",
    "text_annotated_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d865508-e103-4639-9d2b-a3d966aa3f67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 5. Prozess f√ºr das gesamte Korpus ausf√ºhren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f7a78-d268-4b7f-b4fa-0ee9502094b4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stream_texts_from_directory(corpus_filepaths: list[Path]) -> typing.Generator[str, None, None]:\n",
    "    \"\"\"A generator that yields texts from files in the file list one by one.\"\"\"\n",
    "    for filepath in corpus_filepaths:\n",
    "            yield filepath.read_text()\n",
    "\n",
    "def process_corpus(corpus_dir: Path, output_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Reads files from corpus_dir, annotates the files with spacy and writes the result\n",
    "    to the output_dir\n",
    "    :param Path corpus_dir: The directory in which the txt files are saved\n",
    "    :param Path corpus_dir: The directory in which the annotations are written to as csv\n",
    "    \"\"\"\n",
    "    corpus_filepaths = [f for f in corpus_dir.iterdir() if f.is_file() and f.suffix == \".txt\"]\n",
    "\n",
    "    start = time()\n",
    "    for filepath, doc in zip(corpus_filepaths, nlp.pipe(stream_texts_from_directory(corpus_filepaths))):\n",
    "        print(filepath)\n",
    "        # Save the token and lemma information to a dictionary\n",
    "        text_annotated = {}\n",
    "        text_annotated['Token'] = [tok.text for tok in doc]\n",
    "        text_annotated['Lemma'] = [tok.lemma_ for tok in doc]\n",
    "\n",
    "        output_path = output_dir / filepath.with_suffix(\".csv\").name\n",
    "        text_annotated.to_csv(output_path, index=False)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"\"\"Processed {len(corpus_filepaths)} texts with spacy.\n",
    "    Took {round((end - start)  / 60, 4)} minutes in total.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916da74d-283a-4e85-99b6-12679751efbb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<details>\n",
    "  <summary><b>Informationen zum Ausf√ºhren des Notebooks ‚Äì Zum Ausklappen klicken ‚¨áÔ∏è</b></summary>\n",
    "Im folgenden werden alle Textdateien im Korpus heruntergeladen und gespeichert. Daf√ºr sind folgende Schritte n√∂tig:\n",
    "<ol>\n",
    "    <li>Es wird eine List erstellt, die die URLs zu den einzelnen Textdateien beinhaltet.</li>\n",
    "    <li>Die Liste wird als txt-Datei gespeichert.</li>\n",
    "    <li>Alle Dateien aus der Liste werden heruntergeladen und in dem Ordner <i>../data/txt</i> gespeichert.</li>\n",
    "</ol>\n",
    "Sollten die Dateien schon an einem anderen Ort vorhanden sein, k√∂nnen die Dateipfade zu den Ordnern angepasst werden. </br>\n",
    "Des Weiteren wird der Ordner f√ºr die annotierten Dateien angelegt: <i>../data/csv</i>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec66fda-7478-44bb-b915-cc3631a319ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ Create download list \n",
    "github_api_txt_dir_path = \"https://api.github.com/repos/dh-network/quadriga/contents/data/txt\"\n",
    "txt_dir_info = requests.get(github_api_txt_dir_path).json()\n",
    "url_list = [entry[\"download_url\"] for entry in txt_dir_info]\n",
    "\n",
    "# üöÄ Write download list to the\n",
    "url_list_path = Path(\"github_txt_file_urls.txt\")\n",
    "with url_list_path.open('w') as output_txt:\n",
    "    output_txt.write(\"\\n\".join(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19401757-c448-4cf4-9168-cd8f173aaa1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Only execute, if you haven't downloaded the files yet!\n",
    "# üöÄ Download all txt files ‚Äì this step will take a while\n",
    "! wget -i github_txt_file_urls.txt -P ../data/txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d28419-a7d0-45e5-9c5f-9d8faa8ad35a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution",
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# üöÄ Create output folder \n",
    "output_dir = Path(r\"../data/csv\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7cbfa-5285-43c6-a757-6fc8645d1489",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Set path to corpus and output dir\n",
    "corpus_dir = Path(r\"../data/txt/\")\n",
    "output_dir = Path(r\"../data/csv\")\n",
    "\n",
    "# Read, annotate, write \n",
    "process_corpus(corpus_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
